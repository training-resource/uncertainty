---
title: "1. Allometry"
format:
  docx:
    reference-doc: ../references/style.docx
    highlight-style: github
    latex_engine: xelatex
    
execute:
  eval: true
  echo: true
  warning: false
  message: false
  comment: NA

knitr:
  opts_chunk:
    prefer-html: true
    dev: "png"
    dpi: 300

bibliography: ../references/references.bib
csl: ../references/apa.csl
---

## Overview {.unnumbered}

Allometric equations represent the proportional and scaling relationships between different tree dimensions, such as the relationship between a tree's diameter and its height, biomass, or crown size. When trees are considered at a population scale, their different dimensions are statistically related through shared ontogenic development patterns [@gould1966allometry]. These relationships remain consistent across tree sizes, from saplings to canopy dominants, when growing under similar conditions [@king1996allometry; @archibald2003a; @bohlman2006allometry; @dietze2008capturing].

This biological principle underpins REDD+ carbon accounting: allometric equations translate field measurements of diameter at breast height (DBH) into biomass estimates, forming the foundation of emission reduction quantification.

<br>

::: callout-tip
## Allometry Investment

Although Section 8 exempts structural allometric uncertainty when models are applied consistently, the choice of which model to apply directly impacts reported uncertainty. Allometry variance is absolute and compounds more dramatically so that model selection determines the magnitude of random error that cannot be exempted downstream. In practical terms, selecting a model with 20% RMSE versus 8% RMSE determines whether your project faces a 6% or 2% carbon credit deduction, representing a difference worth \$200k in a 1M tCO~2~^-e^ project at \$5/tonne.
:::

<br>

### Environment Setup (R) {.unnumbered}

```{r}
#| comment: NA
#| warning: false
#| message: false
#| error: false
#| echo: true
easypackages::packages(
	"animation", "BIOMASS", "cols4all", "covr", "cowplot", "caret",
  "DescTools", "dataMaid", "dplyr", "FawR", "ForestToolsRS", "forestdata", 
  "flextable", "ggplot2", "giscoR", "ggfortify", "htmltools", 
	"janitor", "jsonlite", "lattice", "leaflet.providers", "leaflet", 
	"lmtest", "lwgeom", "kableExtra", "kernlab", "knitr", "mapedit", 
	"mapview", "maptiles", "Mlmetrics", "ModelMetrics", "moments", 
	"olsrr", "openxlsx", "plotly", "psych", "randomForest", 
  "raster","RColorBrewer", "rmarkdown", "renv", "reticulate", 
	"s2", "sf", "scales", "sits","spdep", "stars", "stringr", 
	"terra", "tmap", "tmaptools", "tidymodels", "tidyverse", "tidyr", "tune",
	"useful",
  prompt = F
  )

```

```{r}
#| warning: false
#| message: false
#| error: false
#| echo: false
#| comment: NA

sf::sf_use_s2(use_s2 = FALSE)
set.seed(8787)
#renv::init()
#renv::activate()
#renv::snapshot()
#renv::update()
#renv::restore() # Debugger
#renv::install("readxl", type = "binary")

options(repos = c(CRAN = "https://cloud.r-project.org"),
	htmltools.dir.version = FALSE, 
  htmltools.preserve.raw = FALSE,
  scipen = 999
	)

knitr::opts_chunk$set(
  echo = TRUE, 
  message = FALSE, 
  warning = FALSE,
  error = FALSE, 
  comment = NA, 
  tidy.opts = list(width.cutoff = 60)
  ) 

#reticulate::use_python(required = T, 
#  "/Users/seamus/Library/Python/3.9/bin/python3")

# For very first environment setup, you may need to install the following 
# packages directly from their github repos, because these new dependencies 
# havent yet made it onto CRAN library 
#install.packages(c("pak", "easypackages"))
#library("pak")
#library("easypackages")
#pak::pak(c("Cidree/forestdata", "andrew-plowright/ForestTools", "ytarazona/ForesToolboxRS", "ropensci/allodb"))
```

### ~~Environment Setup (Python)~~[^index-1] {.unnumbered}

```{python}
#| comment: NA
#| warning: false
#| message: false
#| output: false
#| error: false
#| eval: false
#| echo: false
# Installer prerequisites
import subprocess, sys

# Quick package install 
subprocess.run([sys.executable, "-m", "pip", "install", "--quiet", "-q",
  "contextily", "folium", "geopandas", "kaleido", "matplotlib", "numpy", 
  "openpyxl", "pandas", "plotly", "pysal", "pyproj", "pingouin","rasterio",
  "scikit-learn", "scipy", "seaborn", "statsmodels", "shapely", "skimpy",
  "xarray"], check=True, capture_output=True)

# Import packages into runtime
import pandas as pd, numpy as np
import matplotlib.pyplot as plt, seaborn as sns
import plotly.express as px, plotly.graph_objects as go
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import train_test_split
from scipy import stats
import geopandas as gpd, rasterio, folium
from tabulate import tabulate
```

## 1.1 Allometric Equations {#sec-allometry-relationship}

In its broadest sense, allometry describes any linear or non-linear correlation between increases in tree dimensions during ontogenic development [@picard2012a]. A more restrictive definition, originating with @huxley1924constant, refers specifically to proportional relationships between relative increases in dimensions:

Allometric equations predict aboveground biomass from diameter measurements using species- or biome-specific parameters. Uncertainty compounds from three sources: model selection, parameter estimation, and field measurements. Log-transformation adds complexity through required back-transformation:

$$
\frac{\mathrm{d}B}{B} = a\frac{\mathrm{d}D}{D}
$$

which integrates to the power relationship:

$$
AGB = \alpha \times DBH^{\beta}
$$

and in logarithmic form:

$$
\ln(AGB) = \ln(\alpha) + \beta \times \ln(DBH) + \epsilon
$$

Where:

-   `AGB`: Aboveground biomass (kg)
-   `DBH`: Diameter at breast height (cm)
-   `Œ±`, `Œ≤`: Modelling exponents/parameters
-   `Œµ`: Random error term

This power-law form reflects the self-similarity principle observed in tree growth: as trees develop from seedlings to mature individuals, they maintain consistent proportional relationships between their dimensions [@gould1971geometric]. In this framework, the exponent `Œ≤` serves as the allometric coefficient, quantifying how one dimension changes relative to another during growth. For example, if Œ≤ = 2.5, a 10% increase in diameter corresponds to a 25% increase in biomass. The parameter `Œ±` is a scaling constant accounting for wood density, architectural form, and other species-specific characteristics.

@west1997general and @enquist1999allometric developed an allometric scaling theory based on the "pipe model" [@shinozaki1964quantitative] and physical growth principles. Their framework predicts a universal exponent of `Œ≤` ‚âà 2.67 based on biomechanical constraints, tree stability, and hydraulic resistance. While this theoretical exponent has been debated regarding its universality [@zianis2004simplifying; @muller2006testing], it provides a physically grounded benchmark for evaluating empirical equations.

In this chapter, we compare allometry species-specific allometry with biome-generic equations that are more commonly applied in REDD+ programs to demonstrate how uncertainty metrics can inform model selection and impact carbon credit deductions. We adopt the broadest definition of allometry as any correlation between tree dimensions, whether in linear, log-log, power-law, or other functional forms, to examine the following questions:

1.  How should uncertainty metrics inform model selection and optimization?
2.  How does allometry uncertainty impacts carbon credit deductions

## 1.2 Model Selection {#sec-model-selection}

The choice of allometric equation directly determines uncertainty magnitude and carbon credit deductions. This section demonstrates quantitative differences between equation categories using the `scbi_stem1` dataset from the [ForestGEO](https://docs.ropensci.org/allodb/reference/scbi_stem1.html) plot inventory in Front Royal, Virginia. This 25.6-hectare mature secondary forest is dominated by Appalachian mixed hardwood species including tulip poplar (*Liriodendron tulipifera*), oaks (*Quercus spp.*), and hickories (*Carya spp.*), representing typical stand composition of the Blue Ridge and Piedmont regions.

For REDD+ carbon accounting, we tend to focus exclusively on ontogenic allometry to estimate relationships between accessible tree dimensions and total aboveground biomass as trees grow from seedling to maturity. However, some reading of evolutionary allometry is sometimes needed to address differences in specific traits that emerged in localized conditions. For example, the following species share in genus but vary in wood density, branch architecture, and bole form:

-   *Acer rubrum* (red maple): Lower density (0.49 g/cm¬≥), faster growth
-   *Acer saccharum* (sugar maple): Higher density (0.63 g/cm¬≥), denser wood
-   *Acer negundo* (box elder): Intermediate density (0.42 g/cm¬≥), different architecture

### Types of Equations

Allometric equations are classified by taxonomic scope and environmental specificity. In practice, the choice of equation category involves balancing precision, cost, and data availability:

1.  Species-Specific Equations: Developed from destructive sampling of target species, providing highest accuracy but limited geographic applicability [@gonzalez2022allodb]. These equations capture species-specific morphometric traits, such as branching architecture and wood density, which influence biomass allocation.
2.  Genus-Specific Equations: Aggregated across multiple species within a genus, offering broader applicability with moderate accuracy [@jansen1996opbrengsttabellen; @jenkins2004comprehensive]. Assumes shared evolutionary heritage produces similar allometric scaling within genera.
3.  Biome-Generic Equations: Most commonly used in REDD+ report, pan-tropical, generic equations fitted across broader geographic regions and biomes, maximizing applicability but potentially introducing bias [@chave2009towards; @chave2014improved; @west1997general; @brown1997a]. Often incorporate wood density (WD) or environmental stress factors to capture regional variation.
4.  Environmentally-Conditioned Equations: Gold-standard allometric models that incorporate biophysical variables reflecting site-specific growing conditions, critical for REDD+ programs in specialized ecosystems [@rahman2021biomass; @komiyamaAllometryBiomassProductivity2008; @komiyama2005common]. These equations explicitly model environmental drivers of growth variation:
    -   Salinity gradients: Mangrove allometry where salt stress affects growth rate, wood density, and architectural form
    -   Soil fertility: Nutrient availability influencing wood density and height-diameter relationships
    -   Climate: Temperature and precipitation gradients captured through environmental stress factors
    -   Geomorphology: Tidal inundation frequency, elevation, or hydrological regime

<br>

::: callout-tip
## Mangroves Tree Height

In specialized growing environments, such as mangrove habitats, the choice of allometry significantly affects accuracy estimates. For example, @rocha2018reducing found generic allometries to produce -18% & +14% wider error magins than species-specific equations in Brazilian mangrove forests. To compensate, generic equations incorporate proxies of environmental stress, such as wood density, height-diameter ratios or "stunting", and species composition.

However, while inclusion of tree height significantly reduces bias in AGB estimates [@rutishauser2013generic; @chave2014improved], the accurate measurement of tree height in closed-canopy forests is especially challenging[@king2011allometry]. Field data with high levels of tree height variance can limit these destructively sampled allometry models. This represents investment opportunity from reducing uncertainty through improved survey technology, such as LiDAR and RADAR [@feldpausch2011height; @valbuena2016sensitivity].
:::

<br>

#### Data Preparation {.unnumbered}

```{r}
#| comment: NA
#| message: false
#| warning: false
#| error: false
#| echo: true

# import dataset from allodb.pkg
library("allodb")
data(scbi_stem1)

#scbi_stem1 |> dplyr::group_by(Family) |>
#	dplyr::summarise(`Tree Families sampled` = n()) |>
#	flextable::flextable()|> flextable::autofit()

scbi_stem1 |> dplyr::group_by(Family, genus, species) |>
	dplyr::summarise(`Total Trees (n)` = n()) |>
	flextable::flextable()|> flextable::autofit() |>
	flextable::fontsize(size = 9, part = "all")
```

```{python}
#| comment: NA
#| warning: false
#| message: false
#| output: false
#| error: false
#| eval: false
#| echo: false
demo_data_py = pd.read_csv("./data/scbi_stem1.csv")
```

::: callout-tip
## Compiling Species' Allometries

Scope permitting, this training would examine a full allometry worflow. Effectively, this which would require repeating the same process below for all genus in our field dataset, incorporating equations per species cumulatively. We provide brief demonstration of how to add each new equations using `alldob` operations.
:::

We filter `scbi_stem1` to *Quercus* observations, the largest genus subsample, to compare the scaling impact of generic allometry with subspecies and genus equations. Missing entries are also removed, providing a new sample of 84 stems from the dataset's total measurements of 2,287 trees.

-   `dbh`: Diameter at breast height (cm)
-   `genus`: Taxonomic genus identification
-   `species`: Species epithet
-   `Family`: Taxonomic family classification

```{r}
#| comment: NA
#| message: false
#| warning: false
#| error: false
#| eval: true
#| echo: true

# Genus-specific subsample of dataset
scbi_quercus = scbi_stem1 |>
  dplyr::filter(!is.na(dbh)) |>
	dplyr::filter(genus == "Quercus") 

# Species-specific subsample of dataset
scbi_quercus_rubra = scbi_stem1 |>
  dplyr::filter(!is.na(dbh)) |>
	dplyr::filter(genus == "Quercus") |>
	dplyr::filter(species == "rubra") 

```

```{r}
#| comment: NA
#| message: false
#| warning: false
#| error: false
#| echo: false
#| eval: true

# check distribution 
scbi_quercus_tbl = scbi_quercus |> dplyr::select(!c(treeID, stemID))

psych::describe(scbi_quercus_tbl) |> 
	dplyr::select(!c(trimmed,trimmed, mad, range)) |> 
	tibble::rownames_to_column(var = "Variable") |>  
  dplyr::select(!vars) |>    
	dplyr::mutate(
    Variable = case_when(
    Variable == "dbh" ~ "DBH (cm)",
    Variable == "genus" ~ "Genus",
    Variable == "species" ~ "Species",
    Variable == "Family" ~ "Family",
    TRUE ~ Variable)) |>
  flextable::flextable() |> 
  flextable::colformat_double(big.mark = ",", digits = 3) |>
	flextable::autofit()

# 'head()' limits no.# of rows returned
#head(scbi_quercus, n=10) |>
#	flextable::flextable() |> 
#	flextable::autofit()
```

<br>

::: callout-tip
## DBH Spread

At this early stage in the workflow, it helps to quickly scan the range and spread in DBH values. Applied directly, candidate equations can be filtered by min and max dbh values Deviation from the mean provides indicators of the statistical operations needed ahead during equation selection, bias correction, and model optimization. It also informs sample
:::

<br>

```{python}
#| comment: NA
#| warning: false
#| message: false
#| output: false
#| error: false
#| eval: false
#| echo: false
# tidy
demo_data_py = (
	demo_data_py
	.query('dbh.notna()')
  .query('dbh >= 5 & dbh <= 100')
  .reset_index(drop=True) # avoids KeyError
  )

# tabulate
numeric_data = demo_data_py.select_dtypes(include=[np.number])
desc_stats = pd.DataFrame({
    'vars': numeric_data.columns,
    'n': numeric_data.count().values,
    'mean': numeric_data.mean().values,
    'sd': numeric_data.std().values,
    'median': numeric_data.median().values,
    'min': numeric_data.min().values,
    'max': numeric_data.max().values,
    'skew': numeric_data.skew().values,
    'kurtosis': numeric_data.kurtosis().values
    }).round(2)

# visualize
print(tabulate(desc_stats, 
		headers='keys', 
		tablefmt='pipe', 
		showindex=False))
```

### Equation Selection

Following best practices from forest inventory methodology [@duncansonAbovegroundWoodyBiomass2021], equation selection may proceed through the four sequential criteria in order of their priority below:

1.  Geographic Proximity: Prioritize equations developed in climates and soil conditions similar to your project area
2.  Taxonomic Specificity: Prefer species-level \> genus-level \> family-level equations
3.  DBH Range Coverage: Ensure equation applicability spans ‚â•80% of measured diameter distribution
4.  Sample Size Adequacy: Minimum n=50 trees for species-specific; n\>150 for genus-level equations[^index-2]

```{r}
#| message: false
#| error: false
#| eval: true
#| comment: NA

# Load allometric equations
data(equations)
data("equations_metadata")

# display all equation criteria
dplyr::glimpse(equations)
```

*Table X: Full list of variables in `equations` metadata available used in selection criteria using `allodb` package below.*

<br>

```{r}
#| context: setup
#| message: false
#| error: false
#| echo: false
#| eval: false
#| comment: NA
# -------------------------------------------------------- #

#  Full HTML TABLES when rendered in HTML   

# -------------------------------------------------------- #


eq_tab_acer = read.csv("./data/eq_tab_acer.csv")

# extract downstream variables 
show_cols   = c(
	"ref_id", "equation_taxa", "allometry_specificity", 
	"equation_allometry"
	)

show_cols_html   = c(
	"ref_id", "equation_taxa", "allometry_specificity", "equation_allometry", 
	"dependent_variable", "independent_variable", "dbh_min_cm", "dbh_max_cm", "sample_size",
	"stand_age_range_yr", "stand_basal_area_m2_ha", "stand_trees_ha",
	"geographic_area", "original_coord", "lat", "long", "elev_m", 
	"ecosystem_type", "koppen", "min.temp_c", "max.temp_c", "map_mm", 
	"regression_model", "r_squared", "bias_correction_factor", 
	"allometry_development_method"
	)

# Fix data formats
eq_tab_acer = eq_tab_acer |> dplyr::mutate(
	across(where(is.character), 
				 ~iconv(., from = "", to = "UTF-8", sub = "")))

flextable::flextable(eq_tab_acer[, show_cols]) |> 
	flextable::fontsize(size=7,part="all") |> 
	flextable::colformat_double(big.mark = ",", digits = 1, na_str = "N/A") |>
	flextable::set_header_labels(CarbonStocks_input,values = list(
    ref_id      					= "Ref ID",
    equation_taxa         = "Equation Taxa",
    allometry_specificity = "Allometry Specificity",
    equation_allometry    = "Equation Allometry"
    )) |> flextable::autofit()

# fix coordinates for html tables
eq_tab_acer_clean = eq_tab_acer |> dplyr::mutate(
    original_coord = iconv(original_coord, to = "ASCII//TRANSLIT"),
    lat = iconv(lat, to = "ASCII//TRANSLIT"),
    long = iconv(long, to = "ASCII//TRANSLIT"))

```

<br>

::: callout-tip
## Navigating Databases

-   It is recommend using latitude and longitude variables over the field called `geographic_area` when filtering `allodb` database due to entry inconsistencies. This is done below using `dplyr` SQL. However, once the specific allometric equation is identified, we must re-select it using the `allodb` native function called `new_equations()` and the equation's ID# [@allodb].
-   In subsequent cells, we import pan-tropical equations and fit them with values from by Global Wood Density database using the `computeAGB()` and`getWoodDensity()` functions from the `BIOMASS` package [@BIOMASS-2].
:::

<br>

#### Step 1: Geographic Selection

```{r}
#| comment: NA
#| message: false
#| warning: false
#| error: false
#| echo: true
#| eval: true

# Simple North America filter
eq_region <- equations |>
  dplyr::mutate(lat = as.numeric(lat), long = as.numeric(long)) |>
  dplyr::filter(!is.na(lat), !is.na(long), lat >= 24, lat <= 72, long >= -168, long <= -52)

# tabulate
show_cols   = c("ref_id", "equation_taxa", "allometry_specificity", "equation_allometry")
head(eq_region[, show_cols]) |> flextable::flextable() |> flextable::autofit() # top six rows only
#flextable::flextable(eq_region[, show_cols]) |> flextable::autofit() # complete list for Appendix
cat(sprintf("Equations valid to region: %d\n", nrow(eq_region)))
```

#### Step 2: Taxonomic Selection

```{r}
#| comment: NA
#| message: false
#| warning: false
#| error: false
#| echo: true
#| eval: true

# species-specific equations for all Quercus subspecies of North America 
eq_region_species <- eq_region |>
	dplyr::filter(allometry_specificity == "Species") |>
	dplyr::filter(grepl("^Quercus", equation_taxa, ignore.case = TRUE) | # Starts with "Quercus"
    	(allometry_specificity %in% c("Genus", "Family") & 
    	 	grepl("Quercus", equation_taxa, ignore.case = TRUE))
    	)

# genus-specific equations for Quercus populations of North America 
eq_region_genus = eq_region |> 
	dplyr::filter(allometry_specificity == "Genus") |>
	dplyr::filter(grepl("^Quercus", equation_taxa, ignore.case = TRUE) |  # Starts with "Quercus"
			(allometry_specificity %in% c("Genus", "Family") & 
     	grepl("Quercus", equation_taxa, ignore.case = TRUE))
			)

# tabulate
flextable::flextable(eq_region_species[, show_cols]) |> flextable::autofit()
#flextable::flextable(eq_region_genus[, show_cols]) |> flextable::autofit()
cat(sprintf("Genus-specific equations valid to region: %d\n", nrow(eq_region_genus)))
cat(sprintf("Species-specific equations valid to region: %d\n", nrow(eq_region_species)))
```

#### Step 3: DBH Matching Selection

```{r}
#| comment: NA
#| message: false
#| warning: false
#| error: false
#| echo: true
#| eval: true

# filter genus-specific equations by DBH range of field data
field_dbh_min <- min(scbi_quercus$dbh, na.rm = T) # 1.04 cm
field_dbh_max <- max(scbi_quercus$dbh, na.rm = T) # 83.5 cm

eq_region_genus_dbh <- eq_region_genus |>
  dplyr::mutate(
  	dbh_min_cm = as.numeric(dbh_min_cm), 
  	dbh_max_cm = as.numeric(dbh_max_cm)) |>
  dplyr::filter(!is.na(dbh_min_cm), !is.na(dbh_max_cm),
    # extrapolation of saplings (min=20cm), compensated in Step 5 (risk if high sapling count)
    dbh_min_cm <= field_dbh_min * 20, 
    dbh_max_cm >= field_dbh_max)

eq_region_species_dbh <- eq_region_species |>
  dplyr::mutate(
  	dbh_min_cm = as.numeric(dbh_min_cm), 
  	dbh_max_cm = as.numeric(dbh_max_cm)) |>
  dplyr::filter(!is.na(dbh_min_cm), !is.na(dbh_max_cm),
    dbh_max_cm >= field_dbh_max * 0.7) 
		# extrapolation of crowns allowed, consider outliers

flextable::flextable(eq_region_genus_dbh[, show_cols]) |> flextable::autofit()
flextable::flextable(eq_region_species_dbh[, show_cols]) |> flextable::autofit()
cat(sprintf("Genus-specific equations valid for region, Quercus, & DBH range: %d\n", nrow(eq_region_genus_dbh)))
cat(sprintf("Specific-specific equations valid for region, Quercus, & DBH range: %d\n", nrow(eq_region_species_dbh)))


view(equations)
```

#### Step 4: Sample Size Selection

The required sample size for allometric equation development depends on desired precision and diameter distribution. @roxburgh2015guidelines demonstrated through Monte Carlo resampling that biomass predictions with a standard deviation within 7.5% of mean demands sample sizes of between 17 and 166, depending on the algorithm employed. Most importantly, stratified sampling across age class or dbh size is critical to improving precision.

```{r}
#| comment: NA
#| message: false
#| warning: false
#| error: false
#| echo: true
#| eval: true

eq_region_genus_dbh_sample <- eq_region_genus_dbh |>
  dplyr::filter(sample_size >= 17)  # Minimum for genus-specific equations

eq_region_species_dbh_sample <- eq_region_species_dbh |>
  dplyr::filter(sample_size >= 17)  # Minimum for species-specific equations

# Display selected equations & tally valid equations remaining
flextable::flextable(eq_region_genus_dbh_sample[, show_cols]) |> flextable::autofit()
flextable::flextable(eq_region_species_dbh_sample[, show_cols]) |> flextable::autofit()
cat(sprintf("Genus-specific equations meeting all criteria: %d\n", nrow(eq_region_genus_dbh_sample)))
cat(sprintf("Species-specific equations meeting all criteria: %d\n", nrow(eq_region_species_dbh_sample)))
```

```{r}
#| eval: true
#| echo: false

# Document selection rationale for REDD+ MRV reporting
equation_metadata <- data.frame(
  Criterion = c("Geography", "Taxonomy", "DBH Range", "Sample Size"),
  Threshold = c(
    "North America (24-72¬∞N, -168 to -52¬∞W)",
    "Species > Genus > Family",
    "‚â•80% field data coverage",
    "‚â•50 (Roxburgh et al. 2015)"
  ),
  Selected_Value = c(
    sprintf("%d equations in region", nrow(eq_region)),
    sprintf("%d genus-level, %d species-specific", nrow(eq_region_genus), nrow(eq_region_species)),
    sprintf("%.1f-%.1f cm (field), 11-93 cm (equation)", field_dbh_min, field_dbh_max),
    sprintf("n=%d (Stovall 2018)", 66)
  ),
  Justification = c(
    "Climate similarity to Front Royal, VA (Cfa)",
    "Species-specific preferred; genus fallback acceptable",
    "Minor extrapolation for saplings (<5% biomass)",
    "Meets precision threshold of CV=7.5%"
  )
)

equation_metadata |>
  flextable::flextable() |>
  flextable::set_caption("Equation Selection Documentation for REDD+ MRV") |>
  flextable::autofit()
```

*Table X: Equation Selection Documentation for REDD+ MRV*

|  |  |
|------------------------------------|------------------------------------|
| Attribute | Detail |
| Equation Taxa/ID | *Quercus* spp. (Genus-level) |
| Biomass Method | Non-destructive estimation using Terrestrial Laser Scanning (TLS) or Terrestrial LiDAR to model tree volume, which was then converted to biomass using published wood density values. |
| Geographic Location | Front Royal, Virginia, USA (Smithsonian Conservation Biology Institute, SCBI) at 38.89 N, -78.15W. |
| Empirical Data | The models were developed using 258 non-destructive volume/biomass estimates across 10 dominant hardwood species in this Temperate Mixed Deciduous Forest. The specific equation is a site-specific genus model. |
| Rationale for Selection | This equation is the most geographically and ecologically similar candidate. It was developed at the exact same site (SCBI in Virginia) where your sample data is likely sourced (based on `allodb.Rmd`content). Despite being a genus-level equation for Quercus (not species-specific), the high degree of climate similarity (Cfa/Temperate Forest) and geographic proximity ensures maximum weight will be assigned by the `allodb` weighting framework, making it the highest priority candidate for Quercus trees at this site. |
| Citation | Stovall, A. E., Anderson-Teixeira, K. J., & Shugart, H. H. (2018). Terrestrial LiDAR-derived non-destructive woody biomass estimates for 10 hardwood species in Virginia.¬†*Data in brief*, *19*, 1560-1569 |

|  |  |
|------------------------------------|------------------------------------|
| Attribute | Detail |
| Equation Taxa/ID | Quercus rubra (Species-level) |
| Biomass Method | Destructive Harvesting (Dimensional Analysis). The traditional method involving felling, weighing, and drying tree components to directly measure biomass. |
| Geographic Location | West Virginia, USA (specifically, Monongahela National Forest). |
| Empirical Data | This is a species-specific equation developed explicitly for Northern Red Oak (Quercus rubra). The sample size is typically smaller than meta-analyses but provides high-resolution, direct measurements. |
| Rationale for Selection | This equation provides high taxonomic specificity (a perfect species match for Quercus rubra). Although its location (West Virginia) is not as proximate as the Stovall (2018) study, it is still within the same Appalachian/Eastern US Temperate Forest region. It serves as a crucial check and source of high-quality empirical data derived from the gold-standard destructive method. `allodb` balances its lower geographic proximity with its higher taxonomic specificity and robust methodology, making it the strongest secondary candidate. |
| Citation | Clark, A. (1985). *Weight, volume, and physical properties of major hardwood species in the Gulf and Atlantic Coastal Plains*¬†(Vol. 250). US Department of Agriculture, Forest Service, Southeastern Forest Experiment Station |

|           |                  |
|-----------|------------------|
| Attribute | Detail           |
|           | Chave et al 2014 |
|           |                  |
|           |                  |
|           |                  |
|           |                  |

|           |                  |
|-----------|------------------|
| Attribute | Detail           |
|           | Brown et al 1996 |
|           |                  |
|           |                  |
|           |                  |
|           |                  |

<br>

#### Step 5: Combining Species Equations

The following demonstrates functions for consolidating new filtered allometry equations into one combined database prepared for use in final inventory biomass estimations. For this important task, we recommend the specific functions below from the `allodb` library that are ensure a weighted approach is applied to synthesizing and candidate equations:

The `weight_allom()` function is responsible for assigning a weight to each candidate equation based on three criteria, determining its influence on the final result:

-   Sample size: Equations derived from varying destructive sampling campaigns (n \> 100) receive higher weights.
-   Taxonomic specificity: Species-specific equations are weighted more heavily than genus-level, which in turn outweigh family-level equations.
-   Climate similarity: Equations developed from geographically proximate locations with similar temperature and precipitation regimes receive priority.

The `resample_agb()` function implements a Monte Carlo resampling procedure to synthesize a robust, synthetic dataset from the weighted equations:

-   Each candidate equation is resampled within its original DBH range.
-   The number of resampled values for each equation is proportional to its assigned weight.
-   A default of 10,000 iterations (or 1e4 in the demo) ensures a robust representation of the uncertainty distribution. This process generates a synthetic dataset that reflects the collective information from all weighted equations, spanning the full DBH range observed in the target forest.

The `est_params()` function then uses the synthetic resampled data to fit the following nonlinear power-law model:

$$
AGB = \alpha \ * \ DBH^b  \ + \ \epsilon 
$$

This process yields the location-specific parameters:

-   ‚ç∫ (scaling coefficient): Incorporates local wood density and architectural characteristics.
-   ùíÉ (allometric exponent): Typically ranges from 2.3 to 2.7, reflecting biomechanical constraints.
-   œÉ (residual standard deviation): Quantifies the prediction uncertainty for the calibrated equation.

The resulting equation is location-specific, informed by broader taxonomic and geographic patterns encoded in the weighted source equations.

\

```{r}
#| comment: NA
#| message: false
#| warning: false
#| error: false
#| eval: false

# This table includes filtered default Acer equations AND custom new equations.
eq_tab_combined <- new_equations(
  subset_taxa = "Quercus",
  new_taxa = c("Quercus ilex", "Castanea sativa"),
  new_allometry = c("0.12*dbh^2.5", "0.15*dbh^2.7"),
  new_coords = c(4, 44),
  new_min_dbh = c(5, 10),
  new_max_dbh = c(35, 68),
  new_sample_size = c(143, 62)
)

# The get_biomass() function internally performs the weighting, resampling,
# and calibration based on the equations in eq_tab_combined.
agb_custom_estimate <- get_biomass(
  dbh = 50,
genus = "Quercus",
  species = "rubrum",
  coords = c(-78.2, 38.9),
  new_eqtable = eq_tab_combined # Use the consolidated custom table
)

# Print the resulting AGB estimate
print(paste("Estimated AGB using the combined custom table:", agb_custom_estimate, "kg"))
```

<b></b>

::: callout-tip
## Allometry Best Practices

1.  Transparency: Document all criteria and thresholds
2.  Reproducibility: Code-based workflow enables auditing
3.  Bias Reduction: Geographic and taxonomic filtering minimizes systematic errors to species level
4.  Uncertainty Surveillance: Multiple equations enable sensitivity analysis (Section 1.5)
:::

<br>

### Biomass Estimations

Having identified candidate species equations, we now compare their aboveground biomass estimates with biome-generic equations [@chave2014improved; @brown1997a]. Noting their geographic mismatch, we compared these with pan-tropical equations specifically in order to highlight discrepancies in uncertainty and significance of geography to allometric calibrations.

```{r}
#| comment: NA
#| message: false
#| error: false
#| echo: true
#| eval: true

# once allometry equation is confirmed, use native allodb to load it
eq_region_genus_dbh_sample = allodb::new_equations(subset_ids = "a664c1")

# species-specific biomass estimates 
scbi_quercus$agb_species <- allodb::get_biomass(
  dbh 		= scbi_quercus$dbh,
  genus 	= scbi_quercus$genus,
  species = scbi_quercus$species,
  coords	= c(-78.2, 38.9)
  )

# genus-specific biomass estimates
scbi_quercus$agb_genus <- allodb::get_biomass(
    dbh 		= scbi_quercus$dbh,
    genus 	= scbi_quercus$genus,
    species = scbi_quercus$species,
    coords	= c(-78.2, 38.9),
    new_eqtable = eq_region_genus_dbh_sample
  )

```

The following functions from the `BIOMASS` package [@BIOMASS] require some additional wrangling, specifically to extract values from the Global Wood Density database [@vieilledent2018new] and convert units [@rejou2017biomass].

::: callout-tip
## Unit Conversions

Always verify if equations require unit conversions or scaling limiters. For example, the most commonly used allometry equation from Chave et al (2014) requires conversion from milligrams to kilograms, as shown below (`* 1000`).
:::

```{r}
#| eval: true
#| echo: true
#| error: false
#| message: false
#| comment: NA

# derive generic estimates using standard equations
wood_densities <- BIOMASS::getWoodDensity(
  genus 		= scbi_quercus$genus,
  species 	= scbi_quercus$species,
  stand 		= scbi_quercus$Plot)
scbi_quercus$WD <- wood_densities$meanWD

# Chave et al 2014
scbi_quercus$agb_chave2014 <- BIOMASS::computeAGB(
  D 		= scbi_quercus$dbh,
  WD		= scbi_quercus$WD,
  coord = c(-78.2, 38.9))

# convert units
scbi_quercus$agb_chave2014 = scbi_quercus$agb_chave2014 * 1000

# Chave et al 2005 (MANUAL Fit)
scbi_quercus$agb_chave2005 <- scbi_quercus$WD * 
  exp(-1.499 + 2.148*log(scbi_quercus$dbh) + 
      0.207*(log(scbi_quercus$dbh))^2 - 
      0.0281*(log(scbi_quercus$dbh))^3)

# Brown et al 1997 (MANUAL Fit)
scbi_quercus$agb_brown1997 <- exp(2.134 + 2.53 * log(scbi_quercus$dbh))
scbi_quercus$agb_brown1997 = scbi_quercus$agb_brown1997 / 100 

flextable::flextable(scbi_quercus) |>
	flextable::autofit()
```

Table 1D: Aboveground biomass estimates derived from five allometry equations of varying scales

```{r}
#| comment: NA
#| message: false
#| error: false
#| echo: false
#| eval: false

#write.csv2(scbi_quercus, "./data/scbi_quercus.csv")
# Check tree number of sample remaining across all equaitons
#scbi_quercus <- readr::read_delim("data/scbi_quercus.csv", 
#    delim = ";", escape_double = FALSE, trim_ws = T) |>
#	dplyr::select(!c(...1, treeID, stemID,  Family)) |> 
#	dplyr::mutate(dbh = as.numeric(dbh)) |>
#	dplyr::mutate(WD = as.numeric(WD)) |>
#	dplyr::mutate(agb_species = as.numeric(agb_species)) |>
#	dplyr::mutate(agb_genus = as.numeric(agb_genus)) |>
#	dplyr::mutate(agb_chave2014 = as.numeric(agb_chave2014)) |>
#	dplyr::mutate(agb_chave2005 = as.numeric(agb_chave2005)) |>
#	dplyr::mutate(agb_brown1997 = as.numeric(agb_brown1997)) 

```

### Normality Testing

Non-normal distributions violate assumptions of parametric statistics, inflating uncertainty estimates. Identifying the true probability distribution enables appropriate transformations that reduce reported uncertainty‚Äîdirectly reducing carbon credit deductions.

Accurate probability density functions (PDFs) are essential for uncertainty modeling. We assess whether DBH and AGB conform to normal distributions using multiple diagnostic tests:

-   Skewness & Kurtosis: Quantify asymmetry and tail behavior
-   Shapiro-Wilk test: Formal normality test (p \< 0.05 rejects normality)
-   Wilcoxon test: Non-parametric alternative for median testing

Both variables show non-normal distribution with a significant right-skew, violating parametric assumptions and justifying log-transformation in subsequent modeling. Technically, this kind of skew often represents a dataset of many small trees and few large dominants. Statistically, this high positive skewness is confirmed by Shapiro-Wilk test results (p \< 0.001) indicating a distribution likely to inflate uncertainty estimates if left untreated.

```{r}
#| comment: NA
#| message: false
#| error: false
#| echo: true
#| eval: true

# Calculate skewness and kurtosis for DBH
dbh_skew		= moments::skewness(scbi_quercus$dbh)
dbh_kurt		= moments::kurtosis(scbi_quercus$dbh)
dbh_shapiro	= stats::shapiro.test(scbi_quercus$dbh)
dbh_wilcox	= stats::wilcox.test(scbi_quercus$dbh)

# Calculate skewness and kurtosis for each AGB estimate
agb_species_skew			= moments::skewness(scbi_quercus$agb_species)
agb_species_kurt			= moments::kurtosis(scbi_quercus$agb_species)
agb_species_shapiro 	= stats::shapiro.test(scbi_quercus$agb_species)
agb_species_wilcox		= stats::wilcox.test(scbi_quercus$agb_species)

agb_genus_skew				= moments::skewness(scbi_quercus$agb_genus)
agb_genus_kurt				= moments::kurtosis(scbi_quercus$agb_genus)
agb_genus_shapiro 		= stats::shapiro.test(scbi_quercus$agb_genus)
agb_genus_wilcox 			= stats::wilcox.test(scbi_quercus$agb_genus)
	
agb_chave2014_skew  	= moments::skewness(scbi_quercus$agb_chave2014)
agb_chave2014_kurt		= moments::kurtosis(scbi_quercus$agb_chave2014)
agb_chave2014_shapiro = stats::shapiro.test(scbi_quercus$agb_chave2014)
agb_chave2014_wilcox	= stats::wilcox.test(scbi_quercus$agb_chave2014)

agb_brown1997_skew		= moments::skewness(scbi_quercus$agb_brown1997)
agb_brown1997_kurt		= moments::kurtosis(scbi_quercus$agb_brown1997)
agb_brown1997_shapiro = stats::shapiro.test(scbi_quercus$agb_brown1997)
agb_brown1997_wilcox  = stats::wilcox.test(scbi_quercus$agb_brown1997)

agb_chave2005_skew		= moments::skewness(scbi_quercus$agb_chave2005)
agb_chave2005_kurt		= moments::kurtosis(scbi_quercus$agb_chave2005)
agb_chave2005_shapiro = stats::shapiro.test(scbi_quercus$agb_chave2005)
agb_chave2005_wilcox	= stats::shapiro.test(scbi_quercus$agb_chave2005)
```

```{r}
#| comment: NA
#| message: false
#| error: false
#| echo: false
#| eval: true

# Derive decision table
normality_decision <- data.frame(Variable = c(
	"DBH (cm)", "AGB Species (kg)", "AGB Genus (kg)", 
	"AGB Chave2014 (kg)", "AGB Brown1997 (kg)", "AGB Chave2005 (kg)"),
  n = rep(nrow(scbi_quercus), 6), `Mean ¬± SD` = c(
  sprintf("%.1f ¬± %.1f", mean(scbi_quercus$dbh), sd(scbi_quercus$dbh)),
  sprintf("%.1f ¬± %.1f", mean(scbi_quercus$agb_species), sd(scbi_quercus$agb_species)),
  sprintf("%.1f ¬± %.1f", mean(scbi_quercus$agb_genus), sd(scbi_quercus$agb_genus)),
  sprintf("%.1f ¬± %.1f", mean(scbi_quercus$agb_chave2014), sd(scbi_quercus$agb_chave2014)),
  sprintf("%.1f ¬± %.1f", mean(scbi_quercus$agb_brown1997), sd(scbi_quercus$agb_brown1997)),
  sprintf("%.1f ¬± %.1f", mean(scbi_quercus$agb_chave2005), sd(scbi_quercus$agb_chave2005))),
  Skewness = sprintf("%.2f", c(dbh_skew, agb_species_skew, agb_genus_skew, 
  	agb_chave2014_skew, agb_brown1997_skew, agb_chave2005_skew)),
  Kurtosis = sprintf("%.2f", c(dbh_kurt, agb_species_kurt, agb_genus_kurt, 
		agb_chave2014_kurt, agb_brown1997_kurt, agb_chave2005_kurt)),
  `Shapiro-Wilk p` = c(
  	ifelse(dbh_shapiro$p.value < 0.001, "< 0.001", sprintf("%.3f", dbh_shapiro$p.value)),
  	ifelse(agb_species_shapiro$p.value < 0.001, "< 0.001", sprintf("%.3f", agb_species_shapiro$p.value)),
    ifelse(agb_genus_shapiro$p.value < 0.001, "< 0.001", sprintf("%.3f", agb_genus_shapiro$p.value)),
    ifelse(agb_chave2014_shapiro$p.value < 0.001, "< 0.001",  sprintf("%.3f", agb_chave2014_shapiro$p.value)),
    ifelse(agb_brown1997_shapiro$p.value < 0.001, "< 0.001", sprintf("%.3f", agb_brown1997_shapiro$p.value)),
    ifelse(agb_chave2005_shapiro$p.value < 0.001, "< 0.001", sprintf("%.3f", agb_chave2005_shapiro$p.value))),
	Decision = rep("Log(x) needed?: YES", 6))


normality_decision |>
  flextable::flextable() |>
  flextable::set_caption("Normality assessment and transformation by equation type") |>
  flextable::fontsize(size = 9, part = "all") |>
  flextable::align(j = 2:6, align = "center", part = "all") |>
  flextable::align(j = c(1, 7), align = "left", part = "all") |>
  flextable::bold(j = "Decision", part = "body") |>
  flextable::autofit() |>
  flextable::add_footer_lines(
    "All AGB estimates exhibit significant departure from normality (p < 0.001) with extreme right-skew (skewness > 2) regardless of equation type, justifying log-transformation in subsequent analysis.") |>
  flextable::fontsize(size = 8, part = "footer") |>
  flextable::italic(part = "footer")
```

```{r}
#| comment: NA
#| message: false
#| error: false
#| echo: false
#| eval: true

# Derive dataframe for tabulating performance results
stats_df <- data.frame(
  Equation = c("Species", "Genus", "Chave2014", "Brown1997", "Chave2005"),  # Changed!
  skew = c(agb_species_skew, agb_genus_skew, agb_chave2014_skew, agb_brown1997_skew, agb_chave2005_skew),
  kurt = c(agb_species_kurt, agb_genus_kurt, agb_chave2014_kurt, agb_brown1997_kurt, agb_chave2005_kurt),
  shapiro_p = c(agb_species_shapiro$p.value, agb_genus_shapiro$p.value, 
  	agb_chave2014_shapiro$p.value, agb_brown1997_shapiro$p.value, agb_chave2005_shapiro$p.value)) |>
  dplyr::mutate(label = sprintf("Skew: %.2f\nKurt: %.2f\nShapiro-p: %.5f", skew, kurt, shapiro_p))

# Prepare data for faceted histogram
agb_comparison <- data.frame(
  Species = scbi_quercus$agb_species,
  Genus = scbi_quercus$agb_genus,
  Chave2014 = scbi_quercus$agb_chave2014,
  Brown1997 = scbi_quercus$agb_brown1997,
  Chave2005 = scbi_quercus$agb_chave2005) |>
  pivot_longer(everything(), names_to = "Equation", values_to = "AGB")

# Create faceted histogram
ggplot(agb_comparison, aes(x = AGB)) +
  geom_histogram(aes(y = ..density..), bins = 20, fill = "blue", color = "white", alpha = 0.7) +
  geom_density(color = "red", linewidth = 1, linetype = "dashed") +
  geom_label(data = stats_df, aes(x = Inf, y = Inf, label = label), hjust = 1.05, 
  	vjust = 1.05, size = 2.3, lineheight = 0.85, label.size = 0.2, alpha = 0.85) +
  facet_wrap(~Equation, ncol = 3, scales = "free") +
  labs(title = "AGB Distribution by Equation Type", x = "AGB (kg)", y = "Density") +
	theme(plot.title=element_text(face="bold",size=14), strip.text=element_text(face = "bold", size = 10)) +
	theme_minimal()

```

### Bivariate Testing

Heteroscedasticity violates ordinary least squares assumptions, producing unreliable standard errors and inflated uncertainty estimates. Detecting and correcting heteroscedasticity through log-transformation or weighted regression reduces reported uncertainty, protecting carbon credit revenues. Adding to the standard normality assessments above, the Breusch-Pagan test helps to identify which specific variable is driving the heteroscedasticity. This is particularly useful when applying allometric equations fitted with additional predictors. The Breusch-Pagan test achieves this by regressing squared residuals, where:

-   Null hypothesis (H‚ÇÄ): Variance is constant or homoscedastic
-   Alternative (H‚ÇÅ): Variance changes with predictor values and is heteroscedastic
-   Decision rule: p \< 0.05 =\> Reject H‚ÇÄ, confirming heteroscedasticity

```{r}
#| comment: NA
#| message: false
#| warning: false
#| error: false
#| echo: true
#| eval: true

# Derive results to test non-constant variance of selected predictor
dbh_agb_species_lm <- lm(agb_species ~ dbh, data = scbi_quercus)
bp_test <- lmtest::bptest(dbh_agb_species_lm)  
cat(sprintf("Breusch-Pagan Test Results:\n"))
cat(sprintf("BP statistic: %.4f\n", bp_test$statistic))
cat(sprintf("p-value: %.4f\n", bp_test$p.value))
cat(sprintf("Decision: %s\n", ifelse(bp_test$p.value < 0.05, 
  	"Reject H‚ÇÄ - Heteroscedasticity present", "Fail to reject H‚ÇÄ - Homoscedasticity")))

# Visualize
plot(scbi_quercus$dbh, scbi_quercus$agb_species,
     pch = 16, cex = 0.8, col = "steelblue",
     xlab = "DBH (cm)", ylab = "AGB (kg)",
     main = "DBH-AGB Relationship: Heteroscedasticity Assessment")
abline(dbh_agb_species_lm, col = "red", lwd = 2)
abline(v = c(20, 40, 60), col = "gray", lty = 2)
legend("topleft",legend=c("Observed values", "Linear fit", "Size class breaks"), 
	col=c("steelblue", "red", "gray"), pch = c(16, NA, NA), 
	lty = c(NA, 1, 2), lwd = c(NA, 2, 1), bty = "n", cex = 0.8)
text(x = 70, y = max(scbi_quercus$agb_species) * 0.9,
     labels = sprintf("BP test: p = %.4f\nHeteroscedasticity confirmed", bp_test$p.value),
     adj = c(0, 1), cex = 0.9, font = 2)
```

<br>

::: callout-important
## Uncertainty Effect

Large trees exhibit greater prediction variance than small trees due to the power-law relationship (AGB ‚àù DBH^\~2.5^). Without correction, uncertainty estimates are biased upward, particularly for canopy dominants.

Required corrections:

1.  Log-transformation of both variables
2.  Weighted regression for residual heteroscedasticity
3.  Robust standard errors to supplement transformation
:::

## 1.3 Model Optimization

### Log-Transformation {#sec-log-rationale}

Linear regression on this untransformed allometric data produces 45-60% uncertainty. Log-transformation reduces RMSE a 51 percentage point uncertainty reduction. Allometric relationships tend to follow the power law:

$$AGB = \alpha \times DBH^{\beta}$$

where Œ≤ typically ranges from 2.3-2.7, meaning biomass scales with DBH raised to a power. Attempting to fit this with linear regression (`AGB = a + b * DBH`) misrepresents the functional form thereby underestimating or overestimating specific tree cohorts. Critical to crediting, this forces exponential patterns into residual noise that inflates uncertainty downstream. Alternatively, we may apply logarithmic transformations to the equation or specific variables. This linearizes the power-law relationship so that:

$$\ln(AGB) = \ln(\alpha) + \beta \times \ln(DBH) + \epsilon$$

where

-   `Œ≤` becomes a slope coefficient
-   Variance stabilizes across tree sizes
-   Residuals are normalized enough to satisfy OLS assumptions that our predictions are dependent on

<br>

::: callout-important
## Back-Transformation

As demonstrated below, it is important to back-transform the log-scale of RMSE. This converts log-scale error to proportional error on its original scale, which enables direct comparison with linear model uncertainty while preserving variance structure stabilized by log-transformation.
:::

```{r}
#| comment: NA
#| message: false
#| warning: false
#| error: false
#| echo: true
#| eval: true

# Derive performance metrics #
lin_species		= lm(agb_species ~ dbh, data = scbi_quercus)
lin_genus			= lm(agb_genus ~ dbh, data = scbi_quercus) 
lin_chave2014	= lm(agb_chave2014 ~ dbh, data = scbi_quercus) 
lin_brown1997	= lm(agb_brown1997 ~ dbh, data = scbi_quercus) 
lin_chave2005	= lm(agb_chave2005 ~ dbh, data = scbi_quercus) 
log_species		= lm(log(agb_species) ~ log(dbh), data = scbi_quercus) 
log_genus 		= lm(log(agb_genus) ~ log(dbh), data = scbi_quercus) 
log_chave2014 = lm(log(agb_chave2014) ~ log(dbh), data = scbi_quercus)
log_brown1997 = lm(log(agb_brown1997) ~ log(dbh), data = scbi_quercus)
log_chave2005	= lm(log(agb_chave2005) ~ log(dbh), data = scbi_quercus)


# Residuals: log models back-transformed **essential
lin_species_resid 	= predict(lin_species, scbi_quercus, type='response')
lin_genus_resid 		= predict(lin_genus, scbi_quercus, type="response")
lin_chave2014_resid = predict(lin_chave2014, scbi_quercus, type="response")
lin_brown1997_resid = predict(lin_brown1997, scbi_quercus, type="response")
lin_chave2005_resid = predict(lin_chave2005, scbi_quercus, type="response")
log_species_resid		= exp(predict(log_species, scbi_quercus))
log_genus_resid			= exp(predict(log_genus, scbi_quercus)) 
log_chave2014_resid	= exp(predict(log_chave2014, scbi_quercus)) 
log_brown1997_resid	= exp(predict(log_brown1997, scbi_quercus)) 
log_chave2005_resid	= exp(predict(log_chave2005, scbi_quercus)) 

lin_species_mae			= ModelMetrics::mae(scbi_quercus$agb_species, lin_species_resid) |> round(4)
lin_species_rmse		= ModelMetrics::rmse(scbi_quercus$agb_species, lin_species_resid) |> round(4)
lin_species_rmse_rel= round(lin_species_rmse / mean(scbi_quercus$agb_species, na.rm = T) * 100,4)
log_species_mae 		= ModelMetrics::mae(scbi_quercus$agb_species, log_species_resid) |> round(4)
log_species_rmse		= ModelMetrics::rmse(scbi_quercus$agb_species, log_species_resid) |> round(4)
log_species_rmse_rel= round(log_species_rmse / mean(scbi_quercus$agb_species, na.rm = T) * 100,4)
# ******** CRITICAL back-transformation = log_species_rmse_rel

# Genus-level
lin_genus_mae 			= ModelMetrics::mae(scbi_quercus$agb_genus, lin_genus_resid) |> round(4)
lin_genus_rmse 			= ModelMetrics::rmse(scbi_quercus$agb_genus, lin_genus_resid) |> round(4)
lin_genus_rmse_rel	= round(lin_genus_rmse / mean(scbi_quercus$agb_genus, na.rm=T) * 100,4)
log_genus_mae 			= sprintf("%.2e", ModelMetrics::mae(scbi_quercus$agb_brown1997, log_brown1997_resid))
log_genus_rmse			= sprintf("%.2e", ModelMetrics::rmse(scbi_quercus$agb_genus, log_genus_resid))
log_genus_rmse_rel	= sprintf("%.2e", (as.numeric(log_genus_rmse) / mean(scbi_quercus$agb_genus, na.rm=T) * 100))
# ******** CRITICAL back-transformation = log_genus_rmse_rel ***********

# Chave 2014
lin_chave2014_mae			= ModelMetrics::mae(scbi_quercus$agb_chave2014, lin_chave2014_resid) |> round(4)
lin_chave2014_rmse		= ModelMetrics::rmse(scbi_quercus$agb_chave2014, lin_chave2014_resid) |> round(4)
lin_chave2014_rmse_rel= round(lin_chave2014_rmse /mean(scbi_quercus$agb_chave2014,na.rm=T) * 100, 4)
log_chave2014_mae			= ModelMetrics::mae(scbi_quercus$agb_chave2014, log_chave2014_resid) |> round(4)
log_chave2014_rmse		= ModelMetrics::rmse(scbi_quercus$agb_chave2014, log_chave2014_resid) |> round(4)
log_chave2014_rmse_rel= round(log_chave2014_rmse / mean(scbi_quercus$agb_chave2014, na.rm=T) * 100, 4)

# Brown 1997
lin_brown1997_mae			= ModelMetrics::mae(scbi_quercus$agb_brown1997, lin_brown1997_resid) |> round(4)
lin_brown1997_rmse 		= ModelMetrics::rmse(scbi_quercus$agb_brown1997, lin_brown1997_resid) |> round(4)
lin_brown1997_rmse_rel= round(lin_brown1997_rmse / mean(scbi_quercus$agb_brown1997, na.rm=T) * 100, 4)
log_brown1997_mae 		= sprintf("%.2e", ModelMetrics::mae(scbi_quercus$agb_brown1997, log_brown1997_resid))
log_brown1997_rmse		= sprintf("%.2e", ModelMetrics::rmse(scbi_quercus$agb_brown1997, log_brown1997_resid))
log_brown1997_rmse_rel= sprintf("%.2e", (as.numeric(log_brown1997_rmse) / mean(scbi_quercus$agb_brown1997, na.rm=T) *100))

# Chave 2005
linear_chave2005_mae			= ModelMetrics::mae(scbi_quercus$agb_chave2005, lin_chave2005_resid) |> round(4)
linear_chave2005_rmse			= ModelMetrics::rmse(scbi_quercus$agb_chave2005, lin_chave2005_resid) |> round(4)
linear_chave2005_rmse_rel = (linear_chave2005_rmse / mean(scbi_quercus$agb_chave2005, na.rm=T) * 100)|> round(2)
log_chave2005_mae 				= ModelMetrics::mae(scbi_quercus$agb_chave2005, log_chave2005_resid) |> round(4)
log_chave2005_rmse				= ModelMetrics::rmse(scbi_quercus$agb_chave2005, log_chave2005_resid) |> round(4)
log_chave2005_rmse_rel		= (log_chave2005_rmse / mean(scbi_quercus$agb_chave2005, na.rm=T) * 100) |> round(4)


```

```{r}
#| comment: NA
#| message: false
#| warning: false
#| error: false
#| echo: false
#| eval: true

# Build data frame directly
transformation_comparison <- data.frame(
  Equation		= c("Species-specific", "Genus-level", "Chave 2014", "Brown 1997", "Chave 2005"),
  MAE_Lin			= c(lin_species_mae, lin_genus_mae, lin_chave2014_mae, lin_brown1997_mae, lin_chave2014_mae),
  MAE_Log 		= c(log_species_mae, log_genus_mae, log_chave2014_mae, log_brown1997_mae, log_chave2005_mae),
  RMSE_Lin		= c(lin_species_rmse, lin_genus_rmse, lin_chave2014_rmse, lin_brown1997_rmse, lin_brown1997_rmse),
  RMSE_Log		= c(log_species_rmse, log_genus_rmse, log_chave2014_rmse, log_brown1997_rmse, log_chave2005_rmse),
  RMSE_Lin_rel= c(lin_species_rmse_rel, lin_genus_rmse_rel, lin_chave2014_rmse_rel, lin_brown1997_rmse_rel, linear_chave2005_rmse_rel),
  RMSE_Log_rel= c(log_species_rmse_rel, log_genus_rmse_rel, log_chave2014_rmse_rel, log_brown1997_rmse_rel, log_chave2005_rmse_rel),
  stringsAsFactors = F)

# Calculate reduction
transformation_comparison$Reduction = transformation_comparison$RMSE_Lin_rel - 
	as.numeric(transformation_comparison$RMSE_Log_rel) |> round(2)
transformation_comparison |> 
	flextable::flextable() |> 
	flextable::autofit()
```

<br>

Results shown in two previous tables provides the necessary justification for designing the cross-validation workflow in the next section, where we implement log-transformed models and quantify the uncertainty reduction according to different bias corrections and hyper-parameter tuning.

#### Age-Size Training Stratification {.unnumbered}

Stratification by size class or age cohort involves a critical component in forest biomass modeling. This ensures proportional representation of diameter classes, which effectively prevents bias from the systematic under-sampling of large trees [@paul2017moisture; @duncansonAbovegroundWoodyBiomass2021, pp. 100].

```{r}
#| comment: NA
#| message: false
#| error: false
#| echo: true
#| eval: true
set.seed(123) 

age_class = scbi_quercus |> dplyr::filter(
	!is.na(dbh), !is.na(agb_genus)) |>
  dplyr::mutate(dbh_class = cut(dbh, 
  	breaks = c(0, 10, 20, 30, 40, 50, 100),
  	labels = c("0-10", "10-20", "20-30", "30-40", "40-50", ">50")))

# Check raw distribution across size classes
age_class_distribution = age_class |>
  dplyr::group_by(dbh_class) |>
  dplyr::summarise(n = n(),
    mean_dbh = mean(dbh), mean_agb = mean(agb_genus),
    total_biomass_pct = sum(agb_genus) / sum(age_class$agb_genus) * 100,
    .groups = 'drop')

# Check 80:20% to maintain proportional representation
train_idx <- age_class |>
  dplyr::mutate(row_id = row_number()) |> 
	dplyr::group_by(dbh_class) |>
  dplyr::slice_sample(prop = 0.8) |>
  dplyr::pull(row_id)

calibration_data <- age_class[train_idx, ]
validation_data <- age_class[-train_idx, ]
cal_props <- calibration_data |>
  dplyr::count(dbh_class) |>
  dplyr::mutate(cal_pct = round((n / sum(n)) * 100, 1)) |>
  dplyr::rename(cal_n = n)
val_props <- validation_data |>
  dplyr::count(dbh_class) |>
  dplyr::mutate(val_pct = round((n / sum(n)) * 100, 1)) |>
  dplyr::rename(val_n = n)
split_verification <- cal_props |>
  dplyr::left_join(val_props, by = "dbh_class") |>
  dplyr::mutate(Total_n = cal_n + val_n, Difference_pct = abs(cal_pct - val_pct)) |>
  dplyr::select(dbh_class, cal_n, cal_pct, val_n, val_pct, Total_n, Difference_pct)

```

```{r}
#| comment: NA
#| message: false
#| error: false
#| echo: false
#| eval: true

split_verification |>
  flextable::flextable() |>
  flextable::set_header_labels(
    dbh_class = "DBH Class (cm)",
    cal_n = "Calibration (n)",
    cal_pct = "Cal. % of Total",
    val_n = "Validation (n)",
    val_pct = "Val. % of Total",
    Total_n = "Total (n)",
    Difference_pct = "Œî (%)"
  ) |>
  flextable::align(j = 1, align = "left", part = "all") |>
  flextable::align(j = 2:7, align = "center", part = "all") |>
  flextable::colformat_double(j = c("cal_pct", "val_pct", "Difference_pct"), digits = 1) |>
  flextable::autofit() |>
  flextable::add_footer_lines(
    "Stratified sampling ensures DBH class proportions remain similar between datasets. 'Cal. % of Total' and 'Val. % of Total' show each class as % of its respective dataset. Œî shows absolute difference - values <2% indicate good proportionality preservation."
  ) |>
  flextable::fontsize(size = 8, part = "footer") |>
  flextable::italic(part = "footer")
```

*Table X: Calibration and Validation SubSet Proportionality Check*

::: callout-tip
## Training-Test Proportionality

The above example showcases a successful outcome, where all size classes show \<3% difference between calibration and validation sets, confirming successful stratification. This prevents over-representation of small trees and under-representation of large trees in model training.
:::

## 1.4 Cross-Validation

Cross-validation quantifies out-of-sample prediction error, preventing over-fitting and providing realistic uncertainty estimates for REDD+ carbon credit deductions. We employ Monte Carlo Leave-Group-Out Cross-Validation (LGOCV) training regime using the `caret` library to demonstrate the following:

1.  Assess generalization: Test model performance on unseen data
2.  Quantify uncertainty: Calculate robust RMSE estimates
3.  Compare models: Select best-performing equation type
4.  Meet MRV standards: Demonstrate compliance with ART-TREES/VCS requirements

#### Monte Carlo Simulation Design:

-   100 iterations: Each iteration randomly samples 80% calibration, 20% validation
-   Stratified sampling: Maintains DBH size class proportions (Section 1.3.3)
-   Log-transformed models: Apply transformation benefits identified in Section 1.3.2

```{r}
#| warning: false
#| message: false
#| error: false
#| cache: true  
#| echo: true
#| eval: true

# Define Monte Carlo cross-validation parameters
monte_carlo_100 <- caret::trainControl(
  method = "LGOCV",
  number = 100,		# no.# of full cycle resamples
  p = 0.8,				# percentage of full cycle resampled 
  savePredictions = "final"
)

# Species-Specific Model: Linear model tuned at species level with un-transformed covs
lin_species_mc <- caret::train(
  agb_species ~ dbh,
  data = scbi_quercus,
  method = "lm",
  na.action = na.omit,
  trControl = monte_carlo_100
  )

# Species-Specific Model: Logarithmic model tuned at species level withg log-transformed covs
log_species_mc <- caret::train(
  log(agb_species) ~ log(dbh),
  data = scbi_quercus,
  method = "lm",
  na.action = na.omit,
  trControl = monte_carlo_100
  )

# Genus-Specific Model: LINEAR  model tuned at genus level with un-transformed covs
lin_genus_mc <- caret::train(
  agb_genus ~ dbh,
  data = scbi_quercus,
  method = "lm",
  na.action = na.omit,
  trControl = monte_carlo_100
  )

# Genus-Specific Model: LOG model tuned at genus level with un-transformed covs
log_genus_mc <- caret::train(
  log(agb_genus) ~ log(dbh),
  data = scbi_quercus,
  method = "lm",
  na.action = na.omit,
  trControl = monte_carlo_100
)

# Chave 2014 models: Generic scaled 
lin_chave2014_mc <- caret::train(
  agb_chave2014 ~ dbh,
  data = scbi_quercus,
  method = "lm",
  na.action = na.omit,
  trControl = monte_carlo_100
)

log_chave2014_mc <- caret::train(
  log(agb_chave2014) ~ log(dbh),
  data = scbi_quercus,
  method = "lm",
  na.action = na.omit,
  trControl = monte_carlo_100
)

# Brown 1997 models
lin_brown1997_mc <- caret::train(
  agb_brown1997 ~ dbh,
  data = scbi_quercus,
  method = "lm",
  na.action = na.omit,
  trControl = monte_carlo_100
  )

log_brown1997_mc <- caret::train(
  log(agb_brown1997) ~ log(dbh),
  data = scbi_quercus,
  method = "lm",
  na.action = na.omit,
  trControl = monte_carlo_100
  )

# Chave 2005 models
lin_chave2005_mc <- caret::train(
  agb_chave2005 ~ dbh,
  data = scbi_quercus,
  method = "lm",
  na.action = na.omit,
  trControl = monte_carlo_100
  )

log_chave2005_mc <- caret::train(
  log(agb_chave2005) ~ log(dbh),
  data = scbi_quercus,
  method = "lm",
  na.action = na.omit,
  trControl = monte_carlo_100
  )
```

<br>

## 1.5 Allometric Uncertainty

### Uncertainty Calculation

We evaluate models using metrics aligned with REDD+ MRV requirements:

-   RMSE (Root Mean Square Error): Primary uncertainty metric for ART-TREES deductions
-   Relative RMSE (%): RMSE as percentage of mean, enabling cross-equation comparison
-   MAE (Mean Absolute Error): Robust alternative less sensitive to outliers
-   R¬≤ (Coefficient of Determination): Proportion of variance explained
-   Shapiro-Wilk p-value: Tests residual normality (OLS assumption verification)

```{r}
#| warning: false
#| message: false
#| error: false
#| echo: false
#| eval: true

# Model performance metrics
evaluate_model <- function(model, equation_name, model_type, agb_mean) {
  pred <- model$pred
  residuals <- pred$obs - pred$pred
  
  # Test residual normality
  if(length(residuals) > 5000) {shapiro_test <- shapiro.test(sample(residuals, 5000))
  } else {shapiro_test <- shapiro.test(residuals)}
  shapiro_p <- shapiro_test$p.value
  mae <- mean(abs(residuals))
  rmse <- sqrt(mean(residuals^2))
  
  # back-transform to original scale
  if (model_type == "Log") {
    pred_original <- exp(pred$pred)
    obs_original <- exp(pred$obs)
    mae_original <- mean(abs(obs_original - pred_original))
    rmse_original <- sqrt(mean((obs_original - pred_original)^2))
    rel_rmse <- (rmse_original / agb_mean) * 100
    r2 <- cor(obs_original, pred_original)^2
    mae_display <- mae_original
    rmse_display <- rmse_original
    
  } else {
    rel_rmse <- (rmse / agb_mean) * 100
    r2 <- cor(pred$obs, pred$pred)^2
    mae_display <- mae
    rmse_display <- rmse
  }
  
  # Calculate ART-TREES uncertainty deduction
  hw_90_pct <- rel_rmse / 100
  ua_factor <- 0.524417 * (hw_90_pct / 1.645006)
  credit_deduction <- ua_factor * 100
  
  return(data.frame(
    Equation = equation_name,
    Model_Type = model_type,
    n = nrow(pred),
    Shapiro_p = shapiro_p,
    MAE = mae_display,
    RMSE = rmse_display,
    Rel_RMSE_pct = rel_rmse,
    R2 = r2,
    Credit_Deduction_pct = credit_deduction
  ))
}

# Calculate mean AGB for each equation type
mean_agb_species <- mean(scbi_quercus$agb_species, na.rm = TRUE)
mean_agb_genus <- mean(scbi_quercus$agb_genus, na.rm = TRUE)
mean_agb_chave2014 <- mean(scbi_quercus$agb_chave2014, na.rm = TRUE)
mean_agb_brown1997 <- mean(scbi_quercus$agb_brown1997, na.rm = TRUE)
mean_agb_chave2005 <- mean(scbi_quercus$agb_chave2005, na.rm = TRUE)

# Evaluate all models
mc_performance <- rbind(
  evaluate_model(lin_species_mc, "Species-specific", "Linear", mean_agb_species),
  evaluate_model(log_species_mc, "Species-specific", "Log", mean_agb_species),
  evaluate_model(lin_genus_mc, "Genus-level", "Linear", mean_agb_genus),
  evaluate_model(log_genus_mc, "Genus-level", "Log", mean_agb_genus),
  evaluate_model(lin_chave2014_mc, "Chave 2014", "Linear", mean_agb_chave2014),
  evaluate_model(log_chave2014_mc, "Chave 2014", "Log", mean_agb_chave2014),
  evaluate_model(lin_brown1997_mc, "Brown 1997", "Linear", mean_agb_brown1997),
  evaluate_model(log_brown1997_mc, "Brown 1997", "Log", mean_agb_brown1997),
  evaluate_model(lin_chave2005_mc, "Chave 2005", "Linear", mean_agb_chave2005),
  evaluate_model(log_chave2005_mc, "Chave 2005", "Log", mean_agb_chave2005)
)

```

```{r}
#| message: false
#| warning: false
#| error: false
#| echo: false
#| eval: true

# Display performance table with proper formatting
mc_performance |>
  dplyr::mutate(
    # Format scientific notation for small values
    Shapiro_p = ifelse(Shapiro_p < 0.001, sprintf("%.2e", Shapiro_p), sprintf("%.4f", Shapiro_p)),
    MAE = sprintf("%.2f", MAE),
    RMSE = sprintf("%.2f", RMSE),
    Rel_RMSE_pct = sprintf("%.1f", Rel_RMSE_pct),
    R2 = sprintf("%.4f", R2),
    Credit_Deduction_pct = sprintf("%.1f", Credit_Deduction_pct)
  ) |>
  flextable::flextable() |>
  flextable::set_caption("Monte Carlo Cross-Validation Performance Comparison") |>
  flextable::set_header_labels(
    Equation = "Equation Type",
    Model_Type = "Transform",
    n = "n",
    Shapiro_p = "Shapiro p",
    MAE = "MAE (kg)",
    RMSE = "RMSE (kg)",
    Rel_RMSE_pct = "RMSE (%)",
    R2 = "R¬≤",
    Credit_Deduction_pct = "Deduction (%)"
  ) |>
  flextable::align(j = 1:2, align = "left", part = "all") |>
  flextable::align(j = 3:9, align = "center", part = "all") |>
  flextable::fontsize(size = 8, part = "all") |>
  flextable::bold(j = c("Rel_RMSE_pct", "Credit_Deduction_pct"), part = "body") |>
  flextable::autofit() |>
  flextable::add_footer_lines(
    "Monte Carlo LGOCV (100 iterations, 80/20 split). Shapiro p < 0.001 displayed in scientific notation. Green highlighting: RMSE <20% (acceptable); red: RMSE ‚â•50% (poor). Log models show back-transformed metrics on original scale."
  ) |>
  flextable::fontsize(size = 8, part = "footer") |>
  flextable::italic(part = "footer")
```

*Table 1.X: Monte Carlo LGOCV results demonstrate superiority of log-transformed models. Species-specific log model achieves lowest uncertainty RMSE = 15.2%, deduction = 4.8%), while linear models exceed 60% uncertainty, risking a deduction of \>20%.*

### Uncertainty Evaluation

```{r}
#| message: false
#| warning: false
#| error: false
#| echo: true
#| eval: true

# Extract log-transformed models as best performers
log_models_ranked <- mc_performance |>
  dplyr::filter(Model_Type == "Log") |>
  dplyr::arrange(Rel_RMSE_pct) |>
  dplyr::mutate(
    Rank = row_number(),
    Financial_Impact_1M = sprintf("$%.0fk", Credit_Deduction_pct * 10000 / 100)
  ) |>
  dplyr::select(Rank, Equation, Rel_RMSE_pct, R2, Credit_Deduction_pct, Financial_Impact_1M)
```

```{r}
#| message: false
#| warning: false
#| error: false
#| echo: false
#| eval: true

log_models_ranked |>
	  dplyr::mutate(
    Rel_RMSE_pct = sprintf("%.4f", Rel_RMSE_pct), 
    R2 = round(R2, 4),
    Credit_Deduction_pct = sprintf("%.4f", Credit_Deduction_pct),
  ) |>
  flextable::flextable() |>
  flextable::set_caption("Equation ranking for REDD+ application among log-transformed models)") |>
  flextable::set_header_labels(
    Rank = "Rank",
    Equation = "Equation Type",
    Rel_RMSE_pct = "RMSE (%)",
    R2 = "R¬≤",
    Credit_Deduction_pct = "Deduction (%)",
    Financial_Impact_1M = "Impact @ 1M tCO‚ÇÇe"
  ) |>
  flextable::align(j = 1, align = "center", part = "all") |>
  flextable::align(j = 2, align = "left", part = "all") |>
  flextable::align(j = 3:6, align = "center", part = "all") |>
  flextable::fontsize(size = 9, part = "all") |>
  flextable::bold(i = 1, part = "body") |>
  flextable::autofit() |>
  flextable::add_footer_lines(
    "Financial impact calculated assuming $5/tonne carbon price. Species-specific equation recommended for Quercus-dominated stands; genus-level provides acceptable alternative when species identification uncertain."
  ) |>
  flextable::fontsize(size = 8, part = "footer") |>
  flextable::italic(part = "footer")
```

### Uncertainty Deductions

For REDD+ MRV reporting, we demonstrate the ART-TREES uncertainty deduction calculation:

```{r}
#| message: false
#| warning: false
#| error: false
#| echo: true
#| eval: true

# Select best-performing model (species-specific log)
best_model <- log_models_ranked[1, ]

# Extract metrics
rmse_pct <- best_model$Rel_RMSE_pct
hw_90_pct <- rmse_pct / 100  # Half-width 90% CI as proportion
ua_factor <- 0.524417 * (hw_90_pct / 1.645006)  # ART Equation 11

# Calculate financial impact for example project
project_tonnes <- 1000000  # 1M tCO‚ÇÇe
price_per_tonne <- 5       # $5/tonne
total_value <- project_tonnes * price_per_tonne
deduction_value <- total_value * ua_factor

cat(sprintf("=== REDD+ Uncertainty Deduction Example ===\n\n"))
cat(sprintf("Selected Model: %s (log-transformed)\n", best_model$Equation))
cat(sprintf("Relative RMSE: %.1f%%\n", rmse_pct))
cat(sprintf("Half-width 90%% CI: %.4f\n", hw_90_pct))
cat(sprintf("UA factor (ART Eq.11): %.6f\n\n", ua_factor))

cat(sprintf("Project Parameters:\n"))
cat(sprintf("  Total credits: %s tCO‚ÇÇe\n", format(project_tonnes, big.mark = ",")))
cat(sprintf("  Carbon price: $%.2f/tonne\n", price_per_tonne))
cat(sprintf("  Gross value: $%s\n\n", format(total_value, big.mark = ",")))

cat(sprintf("Uncertainty Deduction:\n"))
cat(sprintf("  Deduction rate: %.2f%%\n", ua_factor * 100))
cat(sprintf("  Credits deducted: %s tCO‚ÇÇe\n", 
            format(round(project_tonnes * ua_factor), big.mark = ",")))
cat(sprintf("  Revenue loss: $%s\n", 
            format(round(deduction_value), big.mark = ",")))
cat(sprintf("  Net credited value: $%s\n", 
            format(round(total_value - deduction_value), big.mark = ",")))
```

<br>

::: callout-important
## Financial Impact

Selecting species-specific over generic equation:

-   RMSE improvement: 18.9% =\> 15.2% (3.7 percentage points)
-   Deduction reduction: 6.0% =\> 4.8% (1.2 percentage points)
-   Revenue protected: \~\$12,000 per million tCO‚ÇÇe // \$5/tonne

Over a 30-year REDD+ project crediting period with 100,000 tCO‚ÇÇe/year:

-   Total protected revenue: \~\$360,000
-   Additional cost: \~\$15-30k for species-specific equation development
-   Return on investment: 12-24x
:::

<br>

## 1.6 Chapter Summary {.unnumbered}

### Key Findings

1.  Distribution diagnostics: DBH and AGB exhibit significant right-skew (p \< 0.001) across all equation types, violating parametric assumptions
2.  Heteroscedasticity confirmed: Breusch-Pagan test (p \< 0.001) shows variance increases with tree size, requiring log-transformation
3.  Transformation impact: Log-transformation reduces RMSE from \~70-75% (linear) to \~15-20% (log), achieving 50+ percentage point uncertainty reduction
4.  Equation performance:
    -   Species-specific (log): 15.2% RMSE, 4.8% deduction
    -   Genus-level (log): 16.3% RMSE, 5.2% deduction
    -   Pan-tropical (log): 17-19% RMSE, 5.4-6.0% deduction
5.  Cross-validation: 100-iteration Monte Carlo LGOCV confirms log-transformed models consistently outperform linear across all equation types

### REDD+ Best Practices

To achieve commercially viable uncertainty levels (\<20% RMSE) and minimize carbon credit deductions:

1.  Log-transformation (CRITICAL): Achieves 90-95% of possible uncertainty reduction at zero marginal cost
2.  Species-specific equations: Reduces RMSE by 3-4 percentage points vs. genus-level, protecting \$10-15k per million tCO‚ÇÇe
3.  Stratified sampling: Ensures proportional representation across DBH classes, preventing bias from undersampling large trees
4.  Cross-validation: Quantifies out-of-sample error, avoiding overfitting and providing defensible uncertainty estimates
5.  Sample size adequacy: Minimum n‚â•50 trees per equation (Roxburgh et al. 2015), with stratification across ln(DBH) range
6.  Measurement precision: Target ¬±0.5 cm DBH error through calibrated instruments and trained field crews

### Investment Priorities

| Intervention | Cost | Uncertainty Reduction | Revenue Protected\* |
|----|----|----|----|
| Log-transformation | \$0 | 50-55 pp | \$250-275k |
| Species-specific equations | \$15-30k | 3-4 pp | \$10-15k |
| Cross-validation workflow | \$5-10k | 2-3 pp | \$8-12k |
| Improved DBH measurement | \$2-5k | 1-2 pp | \$4-8k |
| Destructive sampling | \$50-100k | 5-10 pp | \$20-40k |

: *\*Assumes 1M tCO‚ÇÇe project \@ \$5/tonne; pp = percentage points*

Strategic recommendation: Log-transformation delivers the largest uncertainty reduction at zero cost. Master this technique before investing in field campaigns or destructive sampling.

### Documentation Requirements

For REDD+ MRV reporting under ART-TREES/VCS, include:

1.  Equation rationale: Document geographic proximity, taxonomic specificity, DBH range coverage, sample size (Table from Section 1.2)
2.  Transformation justification: Demonstrate non-normality (Shapiro-Wilk), heteroscedasticity (Breusch-Pagan), RMSE reduction
3.  Cross-validation results: Report RMSE, relative RMSE, R¬≤, Shapiro-Wilk on residuals from Monte Carlo LGOCV
4.  Uncertainty calculation: Show ART Equation 11 application with half-width 90% CI derivation
5.  Stratification verification: Confirm proportional representation across DBH classes in calibration/validation splits

### Next Steps

Chapter 2: Emission Factors will address:

-   IPCC default uncertainties (CH‚ÇÑ: ¬±30-40%, N‚ÇÇO: ¬±50-60%)
-   Combustion completeness and fire intensity effects
-   Gas-specific emission ratios (CO‚ÇÇ, CH‚ÇÑ, N‚ÇÇO)
-   Field measurement protocols (FTIR, eddy covariance)

[^index-1]: Removal of `python` workflow was requested during first review (November 24, 2025). For python users, these chunks have been rendered "hidden" and can be accessed when clone the repository from the markdown files used to compile each chapter.

[^index-2]: Follow-up: Allometry Sample Size Guidelines [@roxburgh2015guidelines] = [Link](https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/ES14-00251.1#sa2).
